## 前端大数据处理

https://cloud.tencent.com/developer/article/1359259 海量短文本场景下的去重算法

###去重

- 采用高效的比较方式
- 去重标准

###如何度量两个文本的相似性

编辑距离、Jaccard 距离、cosine 距离、欧式距离、语义距离等

### simHash 算法

特点：将文本映射为 01 串，相似文本之间的 01 串也相似，只是少数几个位置上的 0 和 1 不一样。

可以计算两个 01 串在多少个位置上不同——汉明距离，用来表征原始文本的相似度。

-> 越相似的文本，对应 simhash 映射得到的 01 串之间的汉明距离越小

#### 介绍

传统的 Hash 算法只负责将原始内容尽量均匀随机地映射为一个签名值，原理上仅相当于伪随机数产生算法。传统的 hash 算法产生的两个签名，如果原始内容在一定概率下是相等的；如果不相等，除了说明原始内容不相等外，不再提供任何信息，因为即使原始内容只相差一个字节，所产生的签名也很可能差别很大。所以传统的 Hash 是无法在签名的维度上来衡量原内容的相似度，而 SimHash 本身属于一种局部敏感哈希算法，它产生的 hash 签名在一定程度上可以表征原内容的相似度。

SimHash 是 Google 在 2007 年发表的论文《Detecting Near-Duplicates for Web Crawling 》中提到的一种指纹生成算法或者叫指纹提取算法，被 Google 广泛应用在亿级的网页去重的 Job 中，作为 locality sensitive hash（局部敏感哈希）的一种，其主要思想是降维。

> 什么是降维？ 举个通俗点的例子，一篇若干数量的文本内容，经过 simhash 降维后，可能仅仅得到一个长度为 32 或 64 位的二进制由 01 组成的字符串，这一点非常相似我们的身份证，试想一下，如果你要在中国 13 亿+的茫茫人海中寻找一个人，如果你不知道这个人的身份证，你可能要提供姓名 ，住址， 身高，体重，性别，等等维度的因素来确定是否为某个人，从这个例子已经能看出来，如果有一个一维的核心条件身份证，那么查询则是非常快速的，如果没有一维的身份证条件，可能综合其他几个非核心的维度，也能确定一个人，但是这种查询则就比较慢了，而通过我们的 SimHash 算法，则就像是给每个人生成了一个身份证，使复杂的事物，能够通过降维来简化。

#### 工作原理

![Simhash工作原理](https://tva1.sinaimg.cn/large/006y8mN6ly1g8ng2jnlnpj30kr0ds415.jpg)

解释：
（1）准备一篇文本
（2）过滤清洗，提取 n 个特征关键词，这步一般用分词的方法实现，关于分词，比较常用的有 IK，mmseg4j，ansj
（3）特征加权，这一步如果有自己针对某个行业的定义的语料库时候可以使用，没有的话，就用分词后的词频即可
（4）对关键词进行 hash 降维 01 组成的签名（上述是 6 位）
（5）然后向量加权，对于每一个 6 位的签名的每一位，如果是 1，hash 和权重正相乘，如果为 0，则 hash 和权重负相乘，至此就能得到每个特征值的向量。
（6）合并所有的特征向量相加，得到一个最终的向量，然后降维，对于最终的向量的每一位如果大于 0 则为 1，否则为 0，这样就能得到最终的 simhash 的指纹签名

举例说明：

![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8ng3nm8yaj30l50dc0uq.jpg)

通过上面的步骤，可以利用 SimHash 算法为每一个文本生成一个向量指纹。

可以使用海明距离判断两篇文本的相似性

权重： 可以使用 tf-idf 等算法

tf-idf： 字词的重要性随着出现次数成正比增加，同时随着它在语料库总的出现频率成反比下降。——用以评估一个字词对于一个文件集或者一个语料库中的其中一份文件的重要程度。主要思想是：

> 如果某个词或短语在一篇文章中出现的频率 TF 高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

#### 海明距离

两个码字的对应比特取值不同的比特数称为这两个码字的海明距离。在一个有效编码集中,任意两个码字的海明距离的最小值称为该编码集的海明距离。举例如下：10101 和 00110 从第一位开始依次有第一位、第四、第五位不同，则海明距离为 3。对于二进制字符串的 a 和 b，海明距离为等于在 a XOR b 运算结果中 1 的个数（普遍算法）。

```javascript
var result1 = simhash(["a", "list", "of", "a", "couple", "of", "tokens"]);
// return1 is [0,0,0,1,0,0,1,1,1,0,1,1,0,1,1,0,1,0,0,0,1,1,0,1,0,1,0,1,0,0,1,0]

var result2 = simhash(["a", "list", "of", "a", "couple", "of", "tokens", "!"]);
// result2 is [0,0,0,1,0,0,0,1,1,0,0,1,0,1,1,0,1,0,0,0,1,1,0,1,0,1,0,1,0,0,1,0]
```

差别 （crc32 hash 处理）

return1: [0,0,0,1,0,0,**1**,1,1,0,**1**,1,0,1,1,0,1,0,0,0,1,1,0,1,0,1,0,1,0,0,1,0]

return2: [0,0,0,1,0,0,**0**,1,1,0,**0**,1,0,1,1,0,1,0,0,0,1,1,0,1,0,1,0,1,0,0,1,0]

上面两个文本的海明距离就是 2

一般在利用 simhash 进行文本相似度比较时，我们认为汉明距离小于 3 的文本是相似的。在长文本下，使用 simHash 算法及使用海明距离来度量文本之间的相似度可以极大降低算法的时间复杂度，并且也能取得很好的去重效果。

#### 海明距离的几何意义

n 位的码字可以用 n 维空间的超立方体的一个顶点来表示。两个码字之间的海明距离就是超立方体两个顶点之间的一条边，而且是这两个顶点之间的最短距离。

#### 海量数据下的海明距离计算

[https://yezhwi.github.io/bigdata/2019/01/16/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D%E4%B9%8BSimHash%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B%E5%92%8C%E5%BA%94%E7%94%A8/](https://yezhwi.github.io/bigdata/2019/01/16/海量数据去重之SimHash算法简介和应用/)

在海量数据几百亿的数量下，效率问题还是没有解决的，因为数据是不断添加进来的，不可能每来一条数据，都要和全库的数据做一次比较，按照这种思路，处理速度会越来越慢，线性增长。

针对这个问题在 Google 的论文中也提出了对应的思路，根据鸽巢原理（也称抽屉原理）：

桌上有十个苹果，要把这十个苹果放到九个抽屉里，无论怎样放，我们会发现至少会有一个抽屉里面至少放两个苹果。这一现象就是我们所说的“抽屉原理”。 抽屉原理的一般含义为：“如果每个抽屉代表一个集合，每一个苹果就可以代表一个元素，假如有 n+1 个元素放到 n 个集合中去，其中必定有一个集合里至少有两个元素。” 抽屉原理有时也被称为鸽巢原理。它是组合数学中一个重要的原理。

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8o3c6ds6zj31ig0j0tco.jpg" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8oay11zxfj31dz0u0qkd.jpg" alt="image-20191106100449273" style="zoom:50%;" />

#### 建立倒排索引

##### 什么是倒排索引？

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8oaw9h368j311w0kc17m.jpg" alt="image-20191106135416439" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8oawgfavdj311k0teww5.jpg" alt="image-20191106135429828" style="zoom:50%;" />

倒排索引创建索引的流程：

1） 首先把所有的原始数据进行编号，形成文档列表

2） 把文档数据进行分词，得到很多的词条，以词条为索引。保存包含这些词条的文档的编号信息。

搜索的过程：

当用户输入任意的词条时，首先对用户输入的数据进行分词，得到用户要搜索的所有词条，然后拿着这些词条去倒排索引列表中进行匹配。找到这些词条就能找到包含这些词条的所有文档的编号。

然后根据这些编号去文档列表中找到文档

#### 存储索引

![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8oase7fzoj30lf0gmjte.jpg)

**存储：**

1. 将一个 64 位的 simhash 签名拆分成 4 个 16 位的二进制码。（图上红色的 16 位）
2. 分别拿着 4 个 16 位二进制码查找当前对应位置上是否有元素。（放大后的 16 位）
3. 对应位置没有元素，直接追加到链表上；对应位置有则直接追加到链表尾端。（图上的 S1 — SN）

**查找：**

1. 将需要比较的 simhash 签名拆分成 4 个 16 位的二进制码。
2. 分别拿着 4 个 16 位二进制码每一个去查找 simhash 集合对应位置上是否有元素。
3. 如果有元素，则把链表拿出来顺序查找比较，直到 simhash 小于一定大小的值，整个过程完成。
4. 在去重时，因为汉明距离小于 3 则为重复文本，那么如果存在 simhash 相似的文本，对于四段 simhash 则至少有一段 simhash 是相同的，所以在去重时对于待判断文本 D，如果 D 中每一段的 simhash 都没有相同的，那么 D 为无重复文本。
